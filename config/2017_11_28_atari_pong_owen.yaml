# -------------------------------------
# RTS Policy Distillation - Config File
# -------------------------------------

# RUN INFO
# --------

env: atari_pong
operation: train  # choices: (train,)

train_params:
  random_seed: 0
  num_timesteps: 40000000
  lr_multiplier: 1.0
  replay_buffer_size: 1000000
  batch_size: 32
  gamma: 0.99
  learning_starts: 50000
  learning_freq: 4
  frame_history_len: 4
  target_update_freq: 10000
  grad_norm_clipping: 10
  lr_schedule:  # can use any variables defined in `main.train`
    - 0: 1e-4 * lr_multiplier
    - num_iterations / 10: 1e-4 * lr_multiplier
    - num_iterations / 2: 5e-5 * lr_multiplier
  exploration_schedule:
    - 0: 1.00
    - 1e6: 0.10
    - num_iterations / 2: 0.01
  normalize_inputs: False
  log_freq: 50  # log every X episodes
  log_recent_rewards: False
  save_model: False
  save_images: False

# MODEL ARCHITECTURE
# ------------------

dqn_arch:
  inputs:
    img_in:
      dtype: uint8
      shape: [84, 84, 4]  # (img_h, img_w, frame_history_len * img_c)

  layers:
    - inputs: [img_in]
      type: conv2d
      num_outputs: 32
      kernel_size: 8
      stride: 4
      activation: relu
    - type: conv2d
      num_outputs: 64
      kernel_size: 4
      stride: 2
      activation: relu
    - type: conv2d
      num_outputs: 64
      kernel_size: 3
      stride: 1
      activation: relu
    - type: fc
      num_outputs: 512
      activation: relu

  outputs:
    action:
      dtype: int32
      shape: [6]  # as determined by `env.action_space.n`
      activation: X
