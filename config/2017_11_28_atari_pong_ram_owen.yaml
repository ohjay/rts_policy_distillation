# -------------------------------------
# RTS Policy Distillation - Config File
# -------------------------------------

# RUN INFO
# --------

env: atari_pong_ram
operation: train  # choices: (train,)

train_params:
  random_seed: 0
  num_timesteps: 40000000
  lr_multiplier: 1.0
  replay_buffer_size: 1000000
  batch_size: 32
  gamma: 0.99
  learning_starts: 50000
  learning_freq: 4
  frame_history_len: 1
  target_update_freq: 10000
  grad_norm_clipping: 10
  lr_schedule:  # can use any variables defined in `main.train`
    - 0: 1e-4 * lr_multiplier
    - num_iterations / 10: 1e-4 * lr_multiplier
    - num_iterations / 2: 5e-5 * lr_multiplier
  exploration_schedule:
    - 0: 0.20
    - 1e6: 0.10
    - num_iterations / 2: 0.01
  normalize_inputs: False
  log_freq: 50  # log every X episodes
  log_recent_rewards: False
  save_model: False
  save_images: False

# MODEL ARCHITECTURE
# ------------------

dqn_arch:
  inputs:
    ram_in:
      dtype: uint8
      shape: [128]  # as determined by `env.observation_space.shape`

  layers:
    - inputs: [ram_in]
      type: fc
      num_outputs: 256
      activation: relu
    - type: fc
      num_outputs: 128
      activation: relu
    - type: fc
      num_outputs: 64
      activation: relu

  outputs:
    action:
      dtype: int32
      shape: [6]  # as determined by `env.action_space.n`
      activation: X
