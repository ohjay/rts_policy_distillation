# -------------------------------------
# RTS Policy Distillation - Config File
# -------------------------------------

# RUN INFO
# --------

env: atari_pong_ram
operation: train  # (train,)

train_params:
  random_seed: 0
  num_timesteps: 40000000
  lr_multiplier: 1.0
  replay_buffer_size: 1000000
  batch_size: 32
  gamma: 0.99
  learning_starts: 50000
  learning_freq: 4
  frame_history_len: 1
  target_update_freq: 10000
  grad_norm_clipping: 10
  exploration_schedule: [(0, 0.20), (5e5, 0.30), (8e5, 0.10), (1e6, 0.01)]  # TODO: isn't used yet
  normalize_inputs: False
  log_freq: 50  # log every X episodes
  log_recent_rewards: False
  save_images: False

# MODEL ARCHITECTURE
# ------------------

dqn_arch:
  inputs:
    ram_in:
      dtype: uint8
      shape: [128]  # as determined by `env.observation_space.shape`

  layers:
    - inputs: [ram_in]
      type: fc
      num_outputs: 256
      activation: relu
    - type: fc
      num_outputs: 128
      activation: relu
    - type: fc
      num_outputs: 64
      activation: relu

  outputs:
    action:
      dtype: int32
      shape: [6]  # as determined by `env.action_space.n`
      activation: X
