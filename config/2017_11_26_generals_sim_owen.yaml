# -------------------------------------
# RTS Policy Distillation - Config File
# -------------------------------------

# RUN INFO
# --------

env: generals_sim
operation: train  # choices: (train,)
algorithm: DQN  # choices: (DQN, A3C)

train_params:
  random_seed: 1
  num_timesteps: 5e7
  lr_multiplier: 0.1
  replay_buffer_size: 1000000
  batch_size: 128
  gamma: 0.00
  learning_starts: 50000
  learning_freq: 1
  frame_history_len: 1
  target_update_freq: 5000
  grad_norm_clipping: 10
  lr_schedule:  # can use any variables defined in `main.train`
    - 0: 2e-3 * lr_multiplier
    - num_iterations / 10: 1e-3 * lr_multiplier
    - num_iterations / 2: 1e-4 * lr_multiplier
  exploration_schedule:
    - 0: 0.80
    - 1e6: 0.20
    - num_iterations / 2: 0.10
    - num_iterations: 0.01
  normalize_inputs: True
  log_freq: 200  # log every X episodes
  log_recent_rewards: False
  save_model: True
  delete_old_models: True
  save_images: True
  evaluate_network: True
  nw_returns_capacity: 5  # during network evaluation, remember returns for X episodes
  evaluate_random: True
  restore_from:
    run_dir: run_12-06__22_20
    iteration: 4949
  restore: False


# ENV-SPECIFIC INFO
# -----------------

generals_sim:
  map_height: 18
  map_width: 18
  map_player_count: 2

reset_kwargs:
  reward_fn_names: [move_made]  # (default) choice of reward function in `reward.py`
  reward_weights: [1.0]
  map_init: empty  # choices: (empty, random)
  player_id: 1
  preprocessors:  # choice of preprocessors from `generals_sim.py`
    - add_dimension

step_kwargs:
  player_id: 1
  preprocessors:
    - add_dimension

get_action_kwargs:
  ensure_valid: False  # ensure that Q values produce a VALID action?

get_random_kwargs:
  semi_valid: True  # choose a random action that is guaranteed to "probably" be valid?
  player_id: 1


# CURRICULUM
# ----------

curriculum:
  _on: True
  accumulate_rewards: True
  min_reward_weight: 0.1  # during scheduling, the weight for the latest reward function will go from MRW -> 1
  schedule:  # {step: {curriculum settings}}
    0:
      reward_fn_name: move_made
      gamma: 0.00
    1e7:
      reward_fn_name: land_dt
      gamma: 0.99
    2e7:
      reward_fn_name: vision_gain
      gamma: 0.99
    3e7:
      reward_fn_name: army_size
      gamma: 0.99


# MODEL ARCHITECTURE
# ------------------

dqn_arch:
  inputs:
    friendly:
      dtype: uint8
      shape: [18, 18, 1]

  layers:
    - inputs: [friendly]
      type: conv2d
      num_outputs: 32
      kernel_size: 3
      stride: 1
      activation: relu
    - type: fc
      num_outputs: 256
      activation: relu

  outputs:
    target_xy:
      dtype: int32
      shape: [324]
      activation: X
      order: 0
    direction:
      dtype: int32
      shape: [4]  # four actions centered around active square
      activation: X
      order: 1
